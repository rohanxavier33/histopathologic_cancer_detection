{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyKsU5rBbuuu"
      },
      "outputs": [],
      "source": [
        "rm -r tuner_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzh2tM0KM_0V"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "cEcU0xwnND5n",
        "outputId": "33263de9-d86b-4692-8ddb-2ef48b03e71e"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjYRGPcbNQQu"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXWNUp3CNWjm"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF4yTAzjNZii"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL26tYeENqYn",
        "outputId": "86886471-e515-473a-ba25-807eb00a3890"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9xU-Ze5NsCc",
        "outputId": "1aea8f6e-22cc-4c50-a0f5-20b2a017af4c"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download -c histopathologic-cancer-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ed8SYzjOFc-"
      },
      "outputs": [],
      "source": [
        "! mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEO1gZ7eOHaO",
        "outputId": "47fe5f5a-45c5-49da-ceca-64156bf6d670"
      },
      "outputs": [],
      "source": [
        "! unzip histopathologic-cancer-detection.zip -d data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43bAdP_nMwnq"
      },
      "source": [
        "**Problem Description:**\n",
        "\n",
        "The challenge is a binary image classification task aimed at detecting metastatic cancer in small image patches extracted from larger digital pathology scans. Specifically, the goal is to develop an algorithm that can accurately classify these patches as either containing metastatic cancer (positive label) or not (negative label). The presence of cancer is determined solely by the presence of tumor tissue within the center 32x32 pixel region of each patch. This task has significant clinical relevance, as it directly addresses the critical issue of cancer metastasis detection.\n",
        "\n",
        "**Data Description:**\n",
        "\n",
        "The dataset is a modified version of the PatchCamelyon (PCam) benchmark dataset. It consists of a large number of small, color pathology images. Will Cukierski. Histopathologic Cancer Detection. https://kaggle.com/competitions/histopathologic-cancer-detection, 2018. Kaggle.\n",
        "\n",
        "-   **Size:** The dataset comprises thousands of image patches, split into training and testing sets.\n",
        "-   **Dimension:** Each image patch has a fixed dimension of 96x96 pixels with 3 color channels (RGB).\n",
        "-   **Structure:**\n",
        "    -   The data (which is all stored in `data/`) is organized into two subdirectories: `train/` and `test/`.\n",
        "    -   The `train` folder contains images for training the model, and the `test` folder contains images for which predictions are to be made.\n",
        "    -   A `train_labels.csv` file provides the ground truth labels for the training images, mapping image IDs to binary labels (0 or 1).\n",
        "    -   The labels are only determined by the center 32x32 pixel region of each image.\n",
        "    -   The outer area of the 96x96 images are provided to support fully convolutional network architectures.\n",
        "-   **Format:** Images are in standard image formats (tif).\n",
        "-   **Class Imbalance:** It's important to note that the dataset exhibits class imbalance, with an unequal distribution of positive and negative samples.\n",
        "-   **No Duplicates:** The Kaggle version of the PCam dataset has been processed to remove duplicate images, ensuring a cleaner training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J30qkfg8tdB3",
        "outputId": "f3eb604d-8f43-48fd-bd82-e65933124780"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas matplotlib seaborn opencv-python albumentations tensorflow keras-tuner scikit-learn Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw_-bPIiMwnt"
      },
      "outputs": [],
      "source": [
        "# --- Standard Libraries ---\n",
        "import os\n",
        "import io\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Cleaner output\n",
        "\n",
        "# --- Data Manipulation & Visualization ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Image Processing & Augmentation ---\n",
        "import cv2\n",
        "import albumentations as A  # Efficient augmentations\n",
        "from albumentations.pytorch import ToTensorV2  # Optional for conversion\n",
        "\n",
        "# --- TensorFlow & Deep Learning ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, losses, metrics, callbacks\n",
        "from tensorflow.keras.applications import ResNet50, EfficientNetB3  # Backbones\n",
        "from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense,\n",
        "                                    Dropout, BatchNormalization)\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Legacy loader\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Hyperparameter Tuning & Utilities ---\n",
        "import keras_tuner as kt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUjiFJwsMwnt",
        "outputId": "f41e50eb-1363-4638-e923-42730a12bfa3"
      },
      "outputs": [],
      "source": [
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6cbI3nuMwnu",
        "outputId": "82e0e311-b81b-4a79-c571-d7bbf0c8fb69"
      },
      "outputs": [],
      "source": [
        "# Load labels and check dataset size\n",
        "train_labels = pd.read_csv('data/train_labels.csv')\n",
        "train_dir = 'data/train'\n",
        "print(f\"Total training samples: {len(train_labels)}\")\n",
        "print(f\"Test directory file count: {len(os.listdir('data/test'))}\")  # Verify test size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABnkBQLsMwnu",
        "outputId": "6d26968e-c4f7-4def-9384-89de2cecbf65"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows of the labels\n",
        "print(train_labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "9lIHPV9oMwnu",
        "outputId": "e1ddc2f6-e65c-4899-a2f3-d3a69acd170b"
      },
      "outputs": [],
      "source": [
        "# Class distribution analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.countplot(x='label', data=train_labels, palette='viridis')\n",
        "\n",
        "# Add percentages to the plot\n",
        "total = len(train_labels)\n",
        "for p in ax.patches:\n",
        "    percentage = f'{100 * p.get_height()/total:.1f}%'\n",
        "    ax.annotate(percentage, (p.get_x() + p.get_width()/2, p.get_height()),\n",
        "                ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "plt.title(f'Class Distribution (Total Samples: {total})', fontsize=14)\n",
        "plt.xlabel('Class (0 = Negative, 1 = Positive)', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjBzBSmGMwnv",
        "outputId": "397cef36-430e-4a27-b038-f07e36b71ab4"
      },
      "outputs": [],
      "source": [
        "# Numerical summary\n",
        "class_counts = train_labels['label'].value_counts()\n",
        "print(f\"Class 0 (Negative): {class_counts[0]} samples ({class_counts[0]/total:.1%})\")\n",
        "print(f\"Class 1 (Positive): {class_counts[1]} samples ({class_counts[1]/total:.1%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioT1yD1BMwnv"
      },
      "outputs": [],
      "source": [
        "# Get positive & negative samples\n",
        "num_samples = 5\n",
        "positive_samples = train_labels[train_labels['label'] == 1].sample(n=num_samples, random_state=42)\n",
        "negative_samples = train_labels[train_labels['label'] == 0].sample(n=num_samples, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "hvmyfvj2Mwnv",
        "outputId": "833cc7fb-5d06-41c9-b0f3-78da857e1992"
      },
      "outputs": [],
      "source": [
        "# Function to plot samples with center region highlighted\n",
        "def plot_class_samples(df, class_label, num_samples=5, title_suffix=\"\"):\n",
        "    samples = df[df['label'] == class_label].sample(n=num_samples, random_state=42)\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
        "    fig.suptitle(f\"Class {class_label} Samples {title_suffix}\", y=1.05, fontsize=16)\n",
        "\n",
        "    for ax, (idx, row) in zip(axes, samples.iterrows()):\n",
        "        img = cv2.imread(os.path.join(train_dir, row['id'] + '.tif'))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Highlight center region\n",
        "        cv2.rectangle(img, (32, 32), (64, 64), (255, 0, 0), 2)  # Red rectangle\n",
        "\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Plot both classes with consistent formatting\n",
        "plot_class_samples(train_labels, 1, title_suffix=\"(Positive - Cancer Present)\")\n",
        "plot_class_samples(train_labels, 0, title_suffix=\"(Negative - No Cancer)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Gn5weVkcMwnv",
        "outputId": "4dc28e27-24a5-4de6-88d0-c94fb38d2fd9"
      },
      "outputs": [],
      "source": [
        "def compare_class_color_distributions(num_samples=100):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Sample images from both classes\n",
        "    pos_samples = train_labels[train_labels['label'] == 1].sample(num_samples)\n",
        "    neg_samples = train_labels[train_labels['label'] == 0].sample(num_samples)\n",
        "\n",
        "    # Load and process images\n",
        "    def get_center_pixels(samples):\n",
        "        pixels = []\n",
        "        for _, row in samples.iterrows():\n",
        "            img = cv2.imread(os.path.join(train_dir, row['id'] + '.tif'))\n",
        "            center_region = img[32:64, 32:64]  # Focus on diagnostic area\n",
        "            pixels.append(center_region.reshape(-1, 3))\n",
        "        return np.vstack(pixels)\n",
        "\n",
        "    pos_pixels = get_center_pixels(pos_samples)\n",
        "    neg_pixels = get_center_pixels(neg_samples)\n",
        "\n",
        "    # Plot distributions\n",
        "    channels = ['Red', 'Green', 'Blue']\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.kdeplot(pos_pixels[:, i], color='r', label='Positive', fill=True)\n",
        "        sns.kdeplot(neg_pixels[:, i], color='b', label='Negative', fill=True)\n",
        "        plt.title(f'{channels[i]} Channel Distribution')\n",
        "        plt.xlabel('Pixel Intensity')\n",
        "        plt.xlim(0, 255)\n",
        "        if i == 0: plt.legend()\n",
        "\n",
        "    plt.suptitle(\"Color Channel Distributions in Center Region (n=100 per class)\", y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_class_color_distributions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ7SPc_uMwnw",
        "outputId": "0dbc3eeb-20c3-4bc5-ca00-12bb2db7f90f"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights for loss function\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels['label']),\n",
        "    y=train_labels['label'].values\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(f\"Class Weights: {class_weights}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udDOz_vXMwnw"
      },
      "outputs": [],
      "source": [
        "def create_augmenter():\n",
        "    return A.Compose([\n",
        "        # Spatial transforms\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            shift_limit=0.05,\n",
        "            scale_limit=0.05,\n",
        "            rotate_limit=15,\n",
        "            p=0.5,\n",
        "            border_mode=cv2.BORDER_CONSTANT\n",
        "        ),\n",
        "\n",
        "        # Color transforms\n",
        "        A.RandomBrightnessContrast(p=0.3),\n",
        "        A.CLAHE(p=0.3),\n",
        "        A.HueSaturationValue(p=0.3),\n",
        "\n",
        "        # Remove CropNonEmptyMaskIfExists\n",
        "        A.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzX2iW1xMwnw",
        "outputId": "f8a26ecd-9fe9-4d82-d37f-7d037f3f75ec"
      },
      "outputs": [],
      "source": [
        "# Split data into train/validation\n",
        "train_df, val_df = train_test_split(\n",
        "    train_labels,\n",
        "    test_size=0.2,\n",
        "    stratify=train_labels['label'],  # Preserve class balance\n",
        "    random_state=42\n",
        ")\n",
        "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akr2ZrPrMwnw",
        "outputId": "e8358e8e-9753-42be-f067-0c673ea82a56"
      },
      "outputs": [],
      "source": [
        "# Verify stratified split\n",
        "def check_split_distribution(train_df, val_df):\n",
        "    train_ratio = train_df['label'].mean()\n",
        "    val_ratio = val_df['label'].mean()\n",
        "    orig_ratio = train_labels['label'].mean()\n",
        "\n",
        "    print(f\"Original positive ratio: {orig_ratio:.4f}\")\n",
        "    print(f\"Train positive ratio:    {train_ratio:.4f}\")\n",
        "    print(f\"Val positive ratio:      {val_ratio:.4f}\")\n",
        "\n",
        "    assert np.isclose(train_ratio, orig_ratio, atol=0.005), \"Train split not stratified!\"\n",
        "    assert np.isclose(val_ratio, orig_ratio, atol=0.005), \"Val split not stratified!\"\n",
        "\n",
        "check_split_distribution(train_df, val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "1e_MPEE5Mwnw",
        "outputId": "f64fca1b-4b0f-46c7-e7e6-25dac061a7cb"
      },
      "outputs": [],
      "source": [
        "def visualize_augmentations(df, n=3):\n",
        "    samples = df.sample(n)\n",
        "    fig, axes = plt.subplots(n, 3, figsize=(15, 4*n))\n",
        "    aug = create_augmenter()\n",
        "\n",
        "    for i, (_, row) in enumerate(samples.iterrows()):\n",
        "        img = cv2.imread(os.path.join(train_dir, row['id'] + '.tif'))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Original (normalized for visualization)\n",
        "        axes[i,0].imshow(img/255.0)\n",
        "        axes[i,0].set_title(f\"Original (Class {row['label']})\")\n",
        "\n",
        "        # Apply augmentation twice\n",
        "        for j in [1, 2]:\n",
        "            augmented = aug(image=img)['image']\n",
        "            # Convert tensor to numpy and denormalize\n",
        "            augmented = augmented.numpy().transpose(1, 2, 0)\n",
        "            augmented = augmented * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "            augmented = np.clip(augmented, 0, 1)\n",
        "\n",
        "            axes[i,j].imshow(augmented)\n",
        "            axes[i,j].set_title(f\"Augmented {j}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_augmentations(train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxrC02ztMwnw"
      },
      "source": [
        "### **Key Considerations for Architecture Design**\n",
        "\n",
        "1.  **Focus on Center Region**:\n",
        "\n",
        "    -   The critical diagnostic region is small (32x32 pixels), so architectures must prioritize local feature extraction over global context.\n",
        "\n",
        "    -   Solutions: **Crop the center region** during preprocessing, use **attention mechanisms**, or design **weighted loss functions** to emphasize the center.\n",
        "\n",
        "2.  **Class Imbalance**:\n",
        "\n",
        "    -   Use **class weights**, **Focal Loss**, or **oversampling** to prevent bias toward the majority class.\n",
        "\n",
        "3.  **Computational Efficiency**:\n",
        "\n",
        "    -   96x96 images are smaller than typical ImageNet resolutions, but transfer learning with pretrained models (e.g., ResNet, EfficientNet) can still work with minor adaptations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcbZ4q2EMwnw"
      },
      "source": [
        "### **Architecture Comparison**\n",
        "\n",
        "| **Model** | **Strengths** | **Weaknesses** | **Use Case** |\n",
        "| --- | --- | --- | --- |\n",
        "| **Simple CNN** | Fast, lightweight | Limited feature hierarchy | Baseline for quick testing |\n",
        "| **ResNet-50 + Attention** | Strong pretrained features, dynamic focus | Computationally heavy | High-accuracy deployment |\n",
        "| **EfficientNet-B3** | Efficient, scalable, good out-of-box performance | Requires tuning for medical images | Resource-constrained environments |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlPHEyBxMwnw"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 1. Simple CNN with Center Crop\n",
        "# ---------------------------\n",
        "def build_simple_cnn():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(96, 96, 3)),\n",
        "        layers.Rescaling(1./255),\n",
        "        layers.Normalization(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            variance=[0.229**2, 0.224**2, 0.225**2]\n",
        "        ),\n",
        "        layers.Cropping2D(cropping=((32, 32), (32, 32))),  # 96x96 → 32x32\n",
        "        layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D(2,2),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=losses.BinaryCrossentropy(),\n",
        "        metrics=[\n",
        "            metrics.AUC(name='auc'),\n",
        "            metrics.Precision(name='prec'),\n",
        "            metrics.Recall(name='rec')\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# 2. ResNet-50 + Spatial Attention\n",
        "# ---------------------------\n",
        "def spatial_attention_block(input_tensor):\n",
        "    \"\"\"Focus attention on center region with learnable weights\"\"\"\n",
        "    # Channel aggregation\n",
        "    avg_pool = layers.GlobalAveragePooling2D(keepdims=True)(input_tensor)\n",
        "    max_pool = layers.GlobalMaxPooling2D(keepdims=True)(input_tensor)\n",
        "    concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "\n",
        "    # Spatial attention with center bias\n",
        "    attention = layers.Conv2D(1, (3,3), padding='same', activation='sigmoid')(concat)\n",
        "\n",
        "    # Add center region prior (32x32 at center)\n",
        "    h, w = K.int_shape(input_tensor)[1], K.int_shape(input_tensor)[2]\n",
        "    center_mask = np.zeros((1, h, w, 1))\n",
        "    cy, cx = h//2, w//2\n",
        "    center_mask[:, cy-16:cy+16, cx-16:cx+16, :] = 0.5  # Soft prior\n",
        "    attention = layers.Add()([attention, tf.constant(center_mask, dtype=tf.float32)])\n",
        "\n",
        "    return layers.Multiply()([input_tensor, attention])\n",
        "\n",
        "def build_resnet_attention():\n",
        "    inputs = layers.Input(shape=(96, 96, 3))\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    x = layers.Normalization(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        variance=[0.229**2, 0.224**2, 0.225**2]\n",
        "    )(x)\n",
        "\n",
        "    base_model = ResNet50(\n",
        "        include_top=False,\n",
        "        input_tensor=x,\n",
        "        pooling=None\n",
        "    )\n",
        "    x = base_model.output\n",
        "\n",
        "    x = spatial_attention_block(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "\n",
        "    # Freezing strategy\n",
        "    for layer in base_model.layers[:150]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[150:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(1e-4),\n",
        "        loss=losses.BinaryFocalCrossentropy(gamma=2),\n",
        "        metrics=[metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# 3. EfficientNet-B3 + Context Preservation\n",
        "# ---------------------------\n",
        "def build_efficientnet():\n",
        "    inputs = layers.Input(shape=(96, 96, 3))\n",
        "\n",
        "    # Context-aware preprocessing\n",
        "    x = layers.RandomCrop(64, 64)(inputs)  # Random context around center\n",
        "    x = layers.Resizing(224, 224)(x)       # Upscale for EfficientNet\n",
        "\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "    x = layers.Normalization(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        variance=[0.229**2, 0.224**2, 0.225**2]\n",
        "    )(x)\n",
        "\n",
        "    base_model = EfficientNetB3(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=x\n",
        "    )\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "\n",
        "    # Partial fine-tuning\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-20]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(1e-3),\n",
        "        loss=losses.BinaryCrossentropy(label_smoothing=0.1),\n",
        "        metrics=[metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AopJ5y13Mwnx"
      },
      "source": [
        "### **Key Features Summary Table**\n",
        "\n",
        "| Model | Input Size | Output Size | Parameters | Key Features |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| **Simple CNN** | 96x96x3 | 32x32x3 | ~100K | Center cropping, lightweight |\n",
        "| **ResNet-50+Attention** | 96x96x3 | 7x7x2048 | ~23M | Spatial attention with center prior, focal loss |\n",
        "| **EfficientNet-B3** | 224x224x3 | 7x7x1536 | ~12M | Context preservation via random crops, label smoothing |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl96hAOjMwnx"
      },
      "outputs": [],
      "source": [
        "def create_dataset(df, augment=False):\n",
        "    \"\"\"Create optimized TF Dataset for TIFF images\"\"\"\n",
        "    image_paths = df['id'].apply(lambda x: f\"data/train/{x}.tif\").tolist()\n",
        "    labels = df['label'].values.astype(np.float32)\n",
        "\n",
        "    # Create dataset from slices\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "\n",
        "    def parse_image(path, label):\n",
        "        \"\"\"Read and decode TIFF image\"\"\"\n",
        "        img_bytes = tf.io.read_file(path)\n",
        "        try:\n",
        "            # Use PIL to decode TIFF\n",
        "            image = tf.py_function(\n",
        "                lambda x: np.array(Image.open(io.BytesIO(x.numpy())).convert('RGB')),\n",
        "                [img_bytes],\n",
        "                tf.uint8\n",
        "            )\n",
        "            image.set_shape([96, 96, 3])  # Enforce expected shape\n",
        "            return image, label\n",
        "        except:\n",
        "            print(f\"Error reading {path}, skipping\")\n",
        "            return tf.zeros([96, 96, 3], dtype=tf.uint8), label\n",
        "\n",
        "    def preprocess(image, label):\n",
        "        \"\"\"Standard preprocessing\"\"\"\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "        image = tf.image.per_image_standardization(image)\n",
        "        return image, label\n",
        "\n",
        "    # Map with proper argument handling\n",
        "    dataset = dataset.map(\n",
        "        lambda p, l: tf.py_function(parse_image, [p, l], [tf.uint8, tf.float32]),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        def augment_fn(image, label):\n",
        "            \"\"\"Augmentation pipeline\"\"\"\n",
        "            image = tf.image.random_flip_left_right(image)\n",
        "            image = tf.image.random_brightness(image, 0.1)\n",
        "            image = tf.image.random_contrast(image, 0.9, 1.1)\n",
        "            return image, label\n",
        "        dataset = dataset.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset.batch(64).prefetch(2)\n",
        "\n",
        "# Create datasets\n",
        "train_ds = create_dataset(train_df, augment=True)\n",
        "val_ds = create_dataset(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "UhW6Zz05Mwnx",
        "outputId": "7640ce68-1061-4afd-9062-836dc43e2196"
      },
      "outputs": [],
      "source": [
        "# Check first batch\n",
        "sample_images, sample_labels = next(iter(train_ds))\n",
        "print(f\"Image batch shape: {sample_images.shape}\")  # Should be (64, 96, 96, 3)\n",
        "print(f\"Label batch shape: {sample_labels.shape}\")  # Should be (64,)\n",
        "\n",
        "# Visualize sample\n",
        "plt.imshow(sample_images[0].numpy().astype('float32'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra9NaV6hwA-h",
        "outputId": "721f797c-b6bc-4b24-fff7-1f6afdc26716"
      },
      "outputs": [],
      "source": [
        "# Select a sample of IDs (here, the first 10)\n",
        "sample_ids = train_df['id'].head(10).tolist()\n",
        "\n",
        "# Construct full file paths and check if they exist\n",
        "for img_id in sample_ids:\n",
        "    path = f\"data/train/{img_id}.tif\"\n",
        "    exists = tf.io.gfile.exists(path)\n",
        "    print(f\"File path: {path} exists: {exists}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose a sample file path\n",
        "sample_path = \"data/train/d6b4fe580210f5ea61b7c109b5c2e5cc734c3010.tif\"\n",
        "\n",
        "# Read the file contents\n",
        "img_bytes = tf.io.read_file(sample_path)\n",
        "\n",
        "# Convert tensor to bytes (using .numpy() because we're in eager mode)\n",
        "img_bytes = img_bytes.numpy()\n",
        "\n",
        "try:\n",
        "    # Attempt to load the image with PIL\n",
        "    img = Image.open(io.BytesIO(img_bytes))\n",
        "    # Convert to RGB (in case it's in a different mode)\n",
        "    img = img.convert(\"RGB\")\n",
        "    # Convert to a NumPy array\n",
        "    img_arr = np.array(img, dtype=np.uint8)\n",
        "    print(\"Image loaded successfully with shape:\", img_arr.shape)\n",
        "except Exception as e:\n",
        "    print(\"Error loading image:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for images, labels in train_ds.take(1):\n",
        "    print(\"Batch image shape:\", images.shape)\n",
        "    print(\"Batch labels shape:\", labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XSNd6pNib0BO",
        "outputId": "7fa41019-9ed6-4f99-decf-fe9c722a65ea"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
        "from tensorflow.keras.applications import ResNet50, EfficientNetB3\n",
        "from tensorflow.keras import backend as K\n",
        "import keras_tuner\n",
        "\n",
        "# ===============================\n",
        "# 1. Model Building Functions\n",
        "# ===============================\n",
        "\n",
        "# ---------------------------\n",
        "# Simple CNN with Center Crop\n",
        "# ---------------------------\n",
        "def build_simple_cnn():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(96, 96, 3)),\n",
        "        layers.Rescaling(1./255),\n",
        "        layers.Normalization(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            variance=[0.229**2, 0.224**2, 0.225**2]\n",
        "        ),\n",
        "        layers.Cropping2D(cropping=((32, 32), (32, 32))),  # 96x96 → 32x32\n",
        "        layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D(2,2),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=losses.BinaryCrossentropy(),\n",
        "        metrics=[\n",
        "            metrics.AUC(name='auc'),\n",
        "            metrics.Precision(name='prec'),\n",
        "            metrics.Recall(name='rec')\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# ResNet-50 + Spatial Attention\n",
        "# ---------------------------\n",
        "def spatial_attention_block(input_tensor):\n",
        "    \"\"\"Focus attention on center region with learnable weights.\"\"\"\n",
        "    # Aggregate channels using average and max pooling\n",
        "    avg_pool = layers.GlobalAveragePooling2D(keepdims=True)(input_tensor)\n",
        "    max_pool = layers.GlobalMaxPooling2D(keepdims=True)(input_tensor)\n",
        "    concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "\n",
        "    # Use (1,1) convolution on the pooled features to generate an attention map\n",
        "    attention = layers.Conv2D(1, (1,1), padding='same', activation='sigmoid')(concat)\n",
        "\n",
        "    # Create a center prior: for a feature map of shape (h, w), highlight the center cell.\n",
        "    h, w = K.int_shape(input_tensor)[1], K.int_shape(input_tensor)[2]\n",
        "    center_mask = np.zeros((1, h, w, 1), dtype=np.float32)\n",
        "    if h is not None and w is not None:\n",
        "        cy, cx = h // 2, w // 2\n",
        "        center_mask[:, cy, cx, :] = 0.5  # Soft prior at the center cell\n",
        "    attention = layers.Add()([attention, tf.constant(center_mask)])\n",
        "\n",
        "    return layers.Multiply()([input_tensor, attention])\n",
        "\n",
        "def build_resnet_attention():\n",
        "    inputs = layers.Input(shape=(96, 96, 3))\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    x = layers.Normalization(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        variance=[0.229**2, 0.224**2, 0.225**2]\n",
        "    )(x)\n",
        "\n",
        "    # Use ResNet50 backbone without pretrained weights (or use 'imagenet' if input size is compatible)\n",
        "    base_model = ResNet50(\n",
        "        include_top=False,\n",
        "        weights=None,\n",
        "        input_tensor=x,\n",
        "        pooling=None\n",
        "    )\n",
        "    x = base_model.output\n",
        "\n",
        "    x = spatial_attention_block(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "\n",
        "    # Freeze early layers for efficiency.\n",
        "    for layer in base_model.layers[:150]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[150:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(1e-4),\n",
        "        loss=losses.BinaryFocalCrossentropy(gamma=2),\n",
        "        metrics=[metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# EfficientNet-B3 + Context Preservation\n",
        "# ---------------------------\n",
        "def build_efficientnet():\n",
        "    inputs = layers.Input(shape=(96, 96, 3))\n",
        "\n",
        "    # Context-aware preprocessing: random crop and resizing.\n",
        "    x = layers.RandomCrop(64, 64)(inputs)  # Crop random 64x64 patch.\n",
        "    x = layers.Resizing(224, 224)(x)        # Resize to 224x224 for EfficientNet.\n",
        "\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "    x = layers.Normalization(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        variance=[0.229**2, 0.224**2, 0.225**2]\n",
        "    )(x)\n",
        "\n",
        "    base_model = EfficientNetB3(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=x\n",
        "    )\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "\n",
        "    # Partial fine-tuning: freeze most layers except the last 20.\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-20]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(1e-3),\n",
        "        loss=losses.BinaryCrossentropy(label_smoothing=0.1),\n",
        "        metrics=[metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ===============================\n",
        "# 2. Data Pipeline\n",
        "# ===============================\n",
        "def create_dataset(df, augment=False):\n",
        "    \"\"\"Create optimized TF Dataset for TIFF images with enforced shape.\"\"\"\n",
        "    image_paths = df['id'].apply(lambda x: f\"data/train/{x}.tif\").tolist()\n",
        "    labels = df['label'].values.astype(np.float32)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "\n",
        "    def parse_image(path, label):\n",
        "        \"\"\"Read and decode TIFF image, enforcing known shape and catching load errors.\"\"\"\n",
        "        img_bytes = tf.io.read_file(path)\n",
        "\n",
        "        def load_img(x):\n",
        "            try:\n",
        "                # Load image with PIL and convert to RGB.\n",
        "                img = Image.open(io.BytesIO(x))\n",
        "                img = img.convert('RGB')\n",
        "                return np.array(img, dtype=np.uint8)\n",
        "            except (UnidentifiedImageError, Exception) as e:\n",
        "                print(f\"Error loading image {path.numpy() if hasattr(path, 'numpy') else path}: {e}\")\n",
        "                # Return a black image if loading fails.\n",
        "                return np.zeros((96, 96, 3), dtype=np.uint8)\n",
        "\n",
        "        image = tf.py_function(load_img, [img_bytes], tf.uint8)\n",
        "        image = tf.reshape(image, [96, 96, 3])  # Explicitly set shape\n",
        "        return image, label\n",
        "\n",
        "    def preprocess(image, label):\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "        image = tf.image.per_image_standardization(image)\n",
        "        return image, label\n",
        "\n",
        "    dataset = dataset.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        def augment_fn(image, label):\n",
        "            image = tf.image.random_flip_left_right(image)\n",
        "            image = tf.image.random_brightness(image, 0.1)\n",
        "            image = tf.image.random_contrast(image, 0.9, 1.1)\n",
        "            return image, label\n",
        "        dataset = dataset.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# (Assumes train_df and val_df are defined DataFrames.)\n",
        "train_ds = create_dataset(train_df, augment=True)\n",
        "val_ds = create_dataset(val_df)\n",
        "\n",
        "# ===============================\n",
        "# 3. Hyperparameter Tuning Across Architectures\n",
        "# ===============================\n",
        "def build_hypermodel(hp):\n",
        "    # Let the tuner choose among the three architectures.\n",
        "    model_type = hp.Choice(\"model_type\", [\"simple_cnn\", \"resnet_attention\", \"efficientnet\"])\n",
        "\n",
        "    if model_type == \"simple_cnn\":\n",
        "        model = build_simple_cnn()\n",
        "        lr = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"LOG\", default=1e-3)\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=lr),\n",
        "            loss=losses.BinaryCrossentropy(),\n",
        "            metrics=[metrics.AUC(name=\"auc\")]\n",
        "        )\n",
        "    elif model_type == \"resnet_attention\":\n",
        "        model = build_resnet_attention()\n",
        "        lr = hp.Float(\"lr\", 1e-5, 1e-3, sampling=\"LOG\", default=1e-4)\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=lr),\n",
        "            loss=losses.BinaryFocalCrossentropy(gamma=2),\n",
        "            metrics=[metrics.AUC(name=\"auc\")]\n",
        "        )\n",
        "    else:  # efficientnet\n",
        "        model = build_efficientnet()\n",
        "        lr = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"LOG\", default=1e-3)\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=lr),\n",
        "            loss=losses.BinaryCrossentropy(label_smoothing=0.1),\n",
        "            metrics=[metrics.AUC(name=\"auc\")]\n",
        "        )\n",
        "    return model\n",
        "\n",
        "tuner = keras_tuner.Hyperband(\n",
        "    build_hypermodel,\n",
        "    objective=\"val_auc\",\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory=\"tuner_dir\",\n",
        "    project_name=\"architecture_comparison\"\n",
        ")\n",
        "\n",
        "# Run the hyperparameter search.\n",
        "tuner.search(train_ds, validation_data=val_ds, epochs=10, verbose=3)\n",
        "\n",
        "# Summarize results.\n",
        "tuner.results_summary()\n",
        "\n",
        "# ===============================\n",
        "# 4. Analyzing Tuning Results\n",
        "# ===============================\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best Hyperparameters:\", best_hp.values)\n",
        "\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "eval_results = best_model.evaluate(val_ds)\n",
        "print(\"Evaluation results on validation dataset:\", eval_results)\n",
        "\n",
        "# Create a summary DataFrame of top trials.\n",
        "trials = tuner.oracle.get_best_trials(num_trials=10)\n",
        "results = []\n",
        "for trial in trials:\n",
        "    hp_values = trial.hyperparameters.values\n",
        "    val_auc = trial.score\n",
        "    results.append({**hp_values, \"val_auc\": val_auc})\n",
        "results_df = pd.DataFrame(results).sort_values(\"val_auc\", ascending=False)\n",
        "print(results_df)\n",
        "\n",
        "# Plot validation AUC by model architecture.\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(results_df[\"model_type\"], results_df[\"val_auc\"])\n",
        "plt.xlabel(\"Model Architecture\")\n",
        "plt.ylabel(\"Validation AUC\")\n",
        "plt.title(\"Validation AUC by Model Architecture\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
